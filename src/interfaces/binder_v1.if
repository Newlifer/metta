#      Interface used by domains to request Event channels and
#      higher-level IDC mechanisms
# 
# The "Binder" is the system service which sets up event channels
# between domains.  The "Binder" is only responsible for the setting up
# and tearing down of event channels; communication over channels
# is performed exclusively by kernel code.  Furthermore, details of the
# communication method to be employed (shared memory buffers, etc.) are
# intended to be hidden behind the interface that builds the proxy,
# rather than dictated by the binder.  In this way different IDC
# mechanisms can coexist transparently on the same machine.

local interface binder_v1
{
    # 
    # Types
    # 

    # The target of a client binding operation is a service access point,
    # or SAP.  A SAP consists of an "ID" for the domain offering
    # the service and a "Port" identifier.

    type card64 id;
    type card64 port;

    # An "ID" will in most cases be a "Domain.ID", but could in principle
    # refer to a group, dynamic resource balancer, or other esoteric concept.
    # The server domain can interpret a "Port" however it chooses.

    # Attempts to connect to a SAP can fail because the SAP is invalid,
    # because the server rejects the connection request, or because
    # insufficient system resources are available.

    enum problem {
        bad_id,           # SAP invalid
        bad_port,         # Port invalid
        server_refused,   # Access denied
        server_error,     # Server sent malformed response
        dying,            # Server domain is dying
        failure           # General failure
    }

    exception error { problem p; }

    # During a bind, further information can be passed using cookies.
    # The interpretation of cookies is up to clients and servers,
    # and might well depend on the server "ID" and "Port".  They
    # could describe shared memory buffers, or anything else.

    record cookie {
        memory_v1.address a;
        opaque value;
    }
    sequence<cookie> cookies;

    # 
    # Connection Setup and Teardown
    # 

    # To connect to a SAP, the active client domain supplies a
    # sequence of channel-pairs and a sequence "clt_cks" of "Cookie"s to
    # be transmitted to the server.  

    # If it succeeds, "ConnectTo" attaches the non-null counts in
    # "events" to appropriate "Channel.Endpoint"s connected to peers
    # in the server "id".  It also returns in "svr_cks" the "Cookie"s
    # received from the server as a result of the connection request.

    # For the case when only a pair of event channels and a single
    # cookie each way are to be used, a convenience call is provided:

    simple_connect(id id, port port, channel_v1.pair endpoints,
        cookie client_cookie, out cookie server_cookie)
        raises (channel_v1.no_slots, error);

    close(channel_v1.endpoint ep)
        raises (channel_v1.invalid);

    close_channels(channel_v1.pairs channels)
        raises (channel_v1.invalid);

    # For each valid endpoint "ep != NULL_EP" (in "chans"), ensure
    # that eventually "ep.state" becomes "Free".  If "ep.state" was 
    # "Connected" on entry, also ensure that "peer(ep).state"
    # eventually becomes "Dead".

    # 
    # Registration
    # 

    # A domain offering services accepts connection requests from
    # the binder over a special callback channel.  The domain
    # responds to requests on this channel as specified in the
    # "BinderCallback" interface.
    # 
    # The callback channel, whose active end is in the binder, is
    # established with the procedure "RegisterDomain".  The domain
    # supplies the events and (in "bufs[0]") the read-write buffer for
    # its end of the IDC channel.

    register_domain(channel_v1.pair channels, in cookie tx_buf, out cookie rx_buf)
        raises (channel_v1.no_slots, error);

    # If it succeeds, "RegisterDomain" connects the callback channel
    # and sets "rx_buf" to the read-only receive buffer for the passive end.
    # For both "bufs", "a" is the base of the buffer and "w" its length.
}
