== Stretch Allocator: implementation details ==

stretch_allocator [privileged]
User API via: stretch_allocator_v1::allocate_stretch()/release_stretch()
Kernel impl: protection_domain_t
- range lists of available virtual memory ranges
+ allocate_stretch
+ release_stretch
* there's a VA_quota implied on all stretches owned by a domain

frame_allocator [privileged]
User API via: frame_allocator_v1
Kernel impl: frame_allocator_t
- range lists of available physical memory ranges
+ allocate_frame
+ free_frame
* There's a frame quota implied on a domain

protection_domain (via stretch_allocator?) [kernel]
- stretches list
+ get stretch for given va?

stretch_driver (bound to stretch) [userspace]
1. map(va, pa, attr) : arrange that the virtual address va maps onto the physical address pa with the
(machine-dependent) PTE attributes attr.
2. unmap(va) : remove the mapping of the virtual address va. Any further access to the address should cause
some form of memory fault.
3. mapping(va) ->  (pa, attr) : retrieve the current mapping of the virtual address va, if any.
--> frame_allocator::allocate_region (uses constraints to allocate specific physmem for stretch)
--> routed to prot_dom mapping api? via syscalls


DRM/GEM memory management details in Linux: http://lwn.net/Articles/283798/


== Glue code ==

To construct portable kernel, at least 3 different architectures should be supported.

* x86
* arm
* hosted
* mips?

Pistachio has an excellent c++ implementation of the kernel - use it as reference.
(e.g. OSdev/L4/l4ka-pistachio/kernel/src/generic)


== Applications ==

Domain startup: closure(methods: Go; state: everything new domain needs to get going)


== IDC ==

Portals - sections of JIT generated code, that performs argument marshalling and request PD switch. Portals are
generated by portal manager, which retrieves portal specifications from Interface Definition files. Portal manager
is free to perform some specific optimizations, like portal short-circuiting, which in some cases can save a network
roundtrip.

== Protection Domain ==

PD: a set of virtual addresses and access rights

in a domain: list of stretches and access define PD


Every application (or a process) is a domain.
Newly created domain possesses at least two stretches for text and data.

By default in Nemesis, all loaded code has RX and data R global stretch permissions. This could be improved.
Code is mapped RXM and data RWM to the corresponding new PD.

modules/Builder
modules/DomainMgrOffer


New PDs created by MMU!

SDom - scheduling domain.


Loading modules creates names in modules> context e.g. modules>Gatekeeper (GatekeeperMod in nemesis).


== Structure of the boot process ==

Since the system is highly modular, explicit code does minimal necessary initialization to start BP and APs.
Most of the initialization is done by components during bootup module loading (which includes dependencies resolution).

First code loaded by the bootloader tries to figure out the bootloader type and prepare for further boot.
During preparation the loader creates bootpage - it contains information about bootstrap in custom format.
Preparations obtain entry point address, which is then called to perform bootstrap, address of bootpage is passed in.

Bootable images can be in different formats. Default GRUB image consists of first stage bootloader as kernel and the
binary blob containing all other components together with second stage loader as a module.

E.g.,
kernel /kickstart
module /kernel-startup
module /system-bootimage


First stage loader: /kickstart

Starts in protected mode, linear addresses equal physical, paging is off.

GRUB (or other bootloader)
+--loader
   +--setup stack
   +--find_loader
   +--loader->prepare()
      +--create bootpage
      +--obtain entry point address
   +--entry point(bootpage address)

First stage loader does not allocate any memory, except the bootpage, which can later be freed or put into memory map.
It relocates the second stage loader to be able to run and kicks it off.


Second stage loader: /kernel-startup

Second stage loader inits CPUs, memory system, paging, interrupts, publicly accessible information and boots the primary domain, which will be the privileged domain during runtime and will start up all other domains as necessary.
Primary domain will be booted with paging enabled mapped to a virtual address already (whole /system-bootimage will be mapped).

entry point
+--k_startup
    +--init IDT
    +--init FPU
    +--init PIC
    +--get nexus ident
    +--parse_cmdline
    +--cpuid
    +--cpu_init_features
       +--init_cache
       +--init_pmctr
    +--clear_infopage
    +--Timer$Enable
    +--init_mem
    +--enable_fpu
    +--k_presume(Primal)

TODO: relate Pistachio SMP startup routines here.

Third stage loader: in /system-bootimage

Primary domain "materializes system domain out of thin air".
It reads the list of system "applications" and components from the boot image and adds them to the domain list.
It also holds the default pervasives list which system domains use during startup.

Uses namespace lookups in nexus - TBD how this works.

Primal
+--init_nexus   (finds load image size and startup heap size)
+--set up pervasives
+--create MMU component
+--init frames allocator module (bootpage still has memory map that is used by frames mod)

Primal runs only on BP.




Physical memory layout during kickstart:

          physical memory
   0  +----------------------+ 0x0000_0000
      |                      |
      +----------------------+ 0x0000_8000
      | bootinfo page        |
      +----------------------+ 0x0000_9000
      | .................... |
 1Mb  +----------------------+ 0x0010_0000 --------> 0x0010_0000 identity mapped
      | kickstart code       |
      +----------------------+ 0x0010_8000
      | kickstart data       |
      +----------------------+ 0x0010_9000 --------> 
      |                      |
      | /kernel-startup      |
      |  module              |
      |                      |
      +----------------------+ 0x0011_1000 --------> 0x0100_0000 mapped at 16mb
      |                      |
      | /system-bootimage    |
      |  module              |
      |                      |
      +----------------------+ 0x0011_f000 --------> placement alloc starts here
      |                      |                       do we need it at all? try to avoid.
      | .................... |
      |                      |


== Glue code: syscalls ==

Available Nemesis syscalls to glue code:

  privileged:
    ntsc_swpipl        // Change interrupt priority level.
    ntsc_entkern       // Enter kernel mode.
    ntsc_leavekern     // Leave kernel mode.
    ntsc_kevent        // Send an event from an interrupt stub.
    ntsc_rti           // Return from an interrupt stub.

Syscalls for metta glue:

  privileged:
    ??sc_activate(vcpu_t*)  Activate a process

  unprivileged:
    sc_return()    Return from activation, reentrancy reenabled and code resumed from the next instruction after call.
    sc_return_resume(context_t*) Return from activation, resuming passed context.
    sc_return_block()  Return from activation and block.
    sc_block()         Block awaiting an event.
    sc_yield()         Relinquish CPU allocation for this period.
    sc_send(int n, uint64_t event)      Send an event.


== Applications: startup ==

From the starting application process viewpoint:

- load application code and data
- read library dependencies
- for missing libraries, load them and their dependencies
- link calls from app to used libraries


Library viewpoint:
- library implements a component interface
- this means per-client data is allocated by calling constructors of the interface
- libraries which do not have per-client data may implement interfaces directly, but i presume this is rare.

A typical application:
- load trader library
- load memory library
- instantiate trader interface
- instantiate memory interface
- allocate something
- use trader to locate a peer
- instantiate a peer interface
- send something to peer via interface instance



--------8<--------imaginary cut line--------8<--------do not cross-------->8--------imaginary cut line-------->8--------

----
All Oberon needs is Single Level Storage (SLS); another innovative feature of the System i5.
There is a "Persistent Oberon" variant that offers an SLS-like framework for those people that want to explore that avenue.

Oberon the language allows direct manipulation of the hardware registers, so Oberon the OS is written entirely
in a high-level language, without the need for assembly code.
----


Device drivers usually run in userspace and are coupled with interrupt handlers - usually implemented inside the same
driver and simply waiting on an interrupt semaphore. Drivers communicate using the same portals, generated from
interface definition files. Usually there is a rich interface defined, not only get bytes/send bytes but much more
semantic information about what is going on and why is being maintained by either a driver or an abstraction level above
it. This allows drivers to make much more educated decisions about, for example, read-ahead strategy to use for
particular clients. This makes things a bit more complicated to implement, but in reality there will be some sort of
more abstract and user-task-oriented metadriver just above more dumb and hardware-oriented "executive drivers".
There are some system components, like security server, that implement trusted computing base and maintain overall
integrity of the system. In case of dynamic reconfiguration and continuous upgrades this is vital. Hotswapping of
components would be highly desirable to have, although at this point this is not planned at all.

Sort of coupled with the dynamic reconfiguration is Dynamic Object Loader - a flexible, JIT-enabled system for building
components from available building blocks like static or dynamic library files, separate object files or source files.
According to requirements it is able to make differently built versions of the same component at different times (or
even at the same time, if this is required). It is similar to
[url=http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.44.2856]OMOS object server[/url].

Higher level components make the user interface part. Some handle input events and make them available as signals to gui
components, some handle drawing in virtual clipped areas called windows, one component posesses exclusive access to the
video hardware, the Compositor, it maintains the scene visibility graph of existing windows, composes them and outputs
via graphics card. This is more complex behind the scenes.

The applications run inside nested virtual machines. Level of nesting depends on level of control needed and amount of
emulation required to be able to run application.


-- rephrase this for Contexts case --
Requests for finding other interfaces go through
security server, which decides what applications should see what. It is not uncommon for application to request, e.g. a
memory_manager interface. Even if app receives it, in reality, all its memory requests go through
logging_debugging_memory_manager and the application is actually being debugged without knowing about it. This can
happen for all application interactions with its environment, which effectively puts every application into a highly
controlled sandbox, if necessary. Which, for an internet-enabled mobile OS is quite a requirement.
